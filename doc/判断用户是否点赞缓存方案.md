
# 判断用户是否点赞缓存方案

判断用户是否为该视频点过赞，查询量较大，有以下三种应对方式

1. 不做缓存 -> DB压力较大
2. 全量缓存 -> 随时间推移，缓存侧存储压力较大
3. 部分缓存 -> 如何判断是否回源？

前两个基本是不能接受的，不予考虑。先说下结论，最终我们选择了下文的方案二

## 方案一：缓存过期不回源

对于部分缓存，判断何时回源是很重要的，若缓存过期时间很短，未查到缓存则接回源，那DB侧压力仍然很大

另一种方法是提高缓存时间，均不回源：

简单计算一下，如果设置30天过期时间，每天300万用户，每人点赞20个视频，假设一个是0.5kb 30*300按10000，10k*20*0.5kb，大约是100Mb

+ 查询缓存成功：表示点过赞，返回true
+ 查询缓存不成功：30天以前点过赞/没点过赞，不回源，直接返回false

DB存储点赞数据时，我们可以在uid与vid间添加唯一索引，插入数据使用`insert on duplicate key update`保证幂等

这个存储量，如果我们把时间过期时间拉到一年，其实也是可以接受的。但问题在于业务是否可以接受一年前点过赞的视频，再次打开时不显示点过赞。

并且因为要为不同的key设置不同过期时间，可以选用string和zset两种形式

+ string形式：key分布较为分散，无法进行统一管理，较为不便
+ zset形式：要过期的时间记为score，异步清除 - 实现较为麻烦

## 方案二：用户维度存储点赞视频，冷热数据分离冷数据回源

该方案参考了 得物 的点赞设计，设计较为巧妙，缓存结构如下：

`like:15`中15为用户uid，3454、723、645均为用户点过赞的视频vid，`minVid`是冷热数据的分界线，低于该值记为冷数据，`ttl`为过期时间

```json
{
  "like:15":{
    "ttl":1653532653,    // 缓存新建或更新时时间戳
    "3454":1,            // 用户近一段时间点赞过的视频id
    "723":1,            // 用户近一段时间点赞过的视频id
    "645":1,            // 用户近一段时间点赞过的视频id
    "minVid":645        // 缓存中最小的视频id，用以区分冷热
  }
}
```

设计思路如下：

1. 用户维度：判断是否点赞业务场景一般是：一个用户对应一堆视频，以用户维度创建缓存，可以大幅减少命令执行次数
2. 冷热数据分离：视频id是自增的，id从大到小对应这上传时间从新到旧，用户刷到旧视频的频率相对较低，可根据视频id进行冷热数据分离
3. 冷数据回源：冷数据回源查DB，并写入缓存，这样就可以保证缓存高效利用而且压力不过太大
4. 冷热阈值：限制hash中的字段数，批量查询时，若发现字段数超过1500则查询结束后重构缓存，取前750个视频id，修改minVid
5. ttl字段：以前每次查看是否要更新缓存时，都要调用TTL命令，执行命令次数翻倍，将TTL写入字段一同查询，可减少命令数
6. 续期：当TTL临近过期(达到2/3时)进行续期

### 业务逻辑

批量获取用户是否点赞：

1. HMGET获取vid1、vid2、vid3、ttl、minVid
   1. 获取失败：不存在该key，重新构建
   2. 获取成功但vid未找到
      1. vid >= minVid ==》未点赞
      2. vid <  minVid ==》冷数据，回源查DB，并写入缓存 
   3. 检查TTL快过期则续期
   4. 检测字段值是否超过阈值，超过则重建
2. 返回结果

用户点赞：更新DB，HSET添加该字段，不存在则构建

用户删除：更新DB，HDEL删除该字段，不做其他处理

优点：
+ 可高效利用缓存且缓存存储量不大
+ DB回源的数据量可以接受

缺点：实现起来较为复杂

## 代码实现

这种缓存模式下需要关注：点赞、取消点赞、批量判断用户是否点赞

我们先实现基本的方法：

1. 构建用户点赞视频缓存 - 相对简单，根据视频id和阈值HMSet即可，代码见`utopia-back\cache\like.go:190`
2. 判断数量是否超过阈值 - 查询字段数，和阈值去做判断
3. 超出阈值重建缓存

超出阈值重建缓存时，其中一半的字段是可以保留的，删除不需要的即可

1. 先取出所有字段，排序后将minVid设置为第750位的vid
2. 调用 构建用户点赞视频缓存 函数，不需要传视频id，通过该函数刷新ttl与minVid即可
3. 调用HMDel删除剩余字段

实现代码见`utopia-back\cache\like.go:271`

在`utopia-back\cache\like.go:271`将 用户点赞视频批量写入缓存 进行封装，用户点赞时进行调用。

判断用户是否批量为视频点赞时，检查ttl与字段数量，需要则调用方法进行重建：

```go
// utopia-back\cache\like.go:138

// IsUserLikedVideos 用户是否为某些视频点赞(批量)
//
// result key为vid  <->  0未点赞；1为点赞；2 冷数据,需回源
//
// state 0 成功；1 不存在该key,需要新建；2 查询失败,需回源
func IsUserLikedVideos(uid uint, videoIds []uint) (result map[uint]int, state int) {
	key := UserLikedVideoKeyV3(uid)

	fields := make([]string, len(videoIds)+2)
	fields[0], fields[1] = sMinVid, sTTL
	// 将整数切片转换为字符串切片
	for i, num := range videoIds {
		fields[i+2] = strconv.FormatInt(int64(num), 10)
	}

	// 查询是否存在
	resHMGet := RDB.HMGet(Ctx, key, fields...)
	if resHMGet.Err() != nil {
		logger.Logger.Error(fmt.Sprintf("IsUserLikedVideos cmd:%v", resHMGet.String()))
		state = 2
		return
	} else {
		logger.Logger.Info(fmt.Sprintf("IsUserLikedVideos cmd:%v", resHMGet.String()))
	}

	sMinVidVal, ok1 := resHMGet.Val()[0].(string)
	sTtlVal, ok2 := resHMGet.Val()[1].(string)
	if !ok1 || !ok2 { // 不存在该key，需重新构建
		state = 1
		return
	}

	minVid, _ := strconv.Atoi(sMinVidVal)
	ttl, _ := strconv.Atoi(sTtlVal)

	state = 0 // 查询成功，state标为0

	vidRes := resHMGet.Val()[2:]
	result = make(map[uint]int, len(videoIds))

	for i, vid := range videoIds {
		if _, ok := vidRes[i].(string); ok { // 查询到点赞
			result[vid] = 1
		} else if vid >= uint(minVid) { // 热数据，用户没点赞
			result[vid] = 0
		} else { // 冷数据，需要回源
			result[vid] = 2
		}
	}

	// 判断是否超过域值，是否需要续期
	judgeRebuildVideoLikedVideos(key, ttl)

	return
}
```

## 待改进

用户点赞时，需要写入like_counts表，此处具有较大的优化空间，倘若每次点赞都回写一次，点赞这种高并发场景会将DB压垮。

改进方案：

1. 接入消息队列，收到点赞消息后异步处理like_counts表
2. 使用消息队列解耦后，以视频id进行聚合，之后批量写入DB，例如vid=13的视频，点赞量达到20后，对应字段直接加20，减少对DB的请求
